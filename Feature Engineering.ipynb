{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "## on mac run `brew install lightgbm` \n",
    "## anythign else refer here https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html#apple-clang\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/application_train.csv\")\n",
    "test_df = pd.read_csv(\"./data/application_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-286c6ff8d1a2>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['DAYS_EMPLOYED'][train_df['DAYS_EMPLOYED'] == 365243] = np.nan\n",
      "<ipython-input-60-286c6ff8d1a2>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['DAYS_EMPLOYED'][test_df['DAYS_EMPLOYED'] == 365243] = np.nan\n"
     ]
    }
   ],
   "source": [
    "## clean up some rows\n",
    "train_df['DAYS_EMPLOYED'][train_df['DAYS_EMPLOYED'] == 365243] = np.nan\n",
    "test_df['DAYS_EMPLOYED'][test_df['DAYS_EMPLOYED'] == 365243] = np.nan\n",
    "train_df['OBS_30_CNT_SOCIAL_CIRCLE'][train_df['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "train_df['OBS_60_CNT_SOCIAL_CIRCLE'][train_df['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "test_df['OBS_30_CNT_SOCIAL_CIRCLE'][test_df['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "test_df['OBS_60_CNT_SOCIAL_CIRCLE'][test_df['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "train_df = train_df[train_df['CODE_GENDER'] != 'XNA']\n",
    "\n",
    "## remove rows with only 1 distinct value\n",
    "empty_columns = []\n",
    "for col in train_df.columns:\n",
    "    if len(train_df[col].unique()) <=1:\n",
    "        empty_columns.append(col)\n",
    "train_df = train_df.drop(empty_columns, axis = 1)\n",
    "test_df = test_df.drop(empty_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## other pre-processing\n",
    "train_df[\"DAYS_BIRTH\"] = train_df[\"DAYS_BIRTH\"] * -1 / 365\n",
    "test_df[\"DAYS_BIRTH\"] = test_df[\"DAYS_BIRTH\"] * -1 / 365\n",
    "categorical_columns = train_df.dtypes[train_df.dtypes == \"object\"].index.tolist()\n",
    "train_df[categorical_columns] = train_df[categorical_columns].fillna(\"XNA\")\n",
    "test_df[categorical_columns] = test_df[categorical_columns].fillna(\"XNA\")\n",
    "train_df[\"REGION_RATING_CLIENT\"] = train_df[\"REGION_RATING_CLIENT\"].astype(\"object\")\n",
    "train_df[\"REGION_RATING_CLIENT_W_CITY\"] = train_df[\n",
    "    \"REGION_RATING_CLIENT_W_CITY\"\n",
    "].astype(\"object\")\n",
    "test_df[\"REGION_RATING_CLIENT\"] = test_df[\"REGION_RATING_CLIENT\"].astype(\"object\")\n",
    "test_df[\"REGION_RATING_CLIENT_W_CITY\"] = test_df[\"REGION_RATING_CLIENT_W_CITY\"].astype(\n",
    "    \"object\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shitty self made encoder\n",
    "encoder_dict = {}\n",
    "for col in list(train_df.columns):\n",
    "    if str(train_df[col].dtypes) == \"object\":\n",
    "        tmp = list(train_df[col].unique())\n",
    "        encoder = {name: i for i, name in enumerate(tmp)}\n",
    "        encoder_dict[col] = encoder\n",
    "        train_df[col] = train_df[col].apply(lambda x: encoder[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307507, 122)\n",
      "(48744, 121)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "target_df = train_df.pop(\"TARGET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using LightGBM for recursive feature selection\n",
    "def cut_down_features(train_df, test_df):\n",
    "    num_folds = 3\n",
    "    impt_cols = set()\n",
    "    score = 1\n",
    "    i = 1\n",
    "    while score > 0.72:\n",
    "        selection_data = train_df.drop(list(impt_cols), axis=1)\n",
    "        fold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=33)\n",
    "        score = 0\n",
    "        model_feature_importance = np.zeros_like(selection_data.columns)\n",
    "        for fold_num, (train_indices, val_indices) in enumerate(\n",
    "            fold.split(selection_data, target_df), 1\n",
    "        ):\n",
    "            x_train = selection_data.iloc[train_indices]\n",
    "            x_val = selection_data.iloc[val_indices]\n",
    "            y_train = target_df.iloc[train_indices]\n",
    "            y_val = target_df.iloc[val_indices]\n",
    "            lg = LGBMClassifier(n_jobs=-1, random_state=69)\n",
    "            lg.fit(x_train, y_train)\n",
    "            model_feature_importance += lg.feature_importances_ / num_folds\n",
    "            score += roc_auc_score(y_val, lg.predict_proba(x_val)[:, 1]) / num_folds\n",
    "        imp_cols_indices = np.where(np.abs(model_feature_importance) > 0)\n",
    "        cols_imp = train_df.columns[imp_cols_indices]\n",
    "        if score > 0.7:\n",
    "            impt_cols.update(cols_imp)\n",
    "        i += 1\n",
    "    impt_cols = list(impt_cols)\n",
    "    train_df = train_df[impt_cols]\n",
    "    test_df = test_df[impt_cols]\n",
    "    with open(\"final_cols.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_df.columns.tolist(), f)\n",
    "    gc.collect()\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "reduced_train_df, reduced_test_df = cut_down_features(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.columns))\n",
    "print(len(reduced_train_df.columns))\n",
    "# lmao\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reduced_train_df, target_df, test_size=0.2\n",
    ")\n",
    "\n",
    "data_matrix = xgb.DMatrix(data=reduced_train_df, label=target_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.306845591800237\n",
      "[0.23833515 0.2137008  0.24095054 ... 0.22724384 0.24410793 0.242542  ]\n"
     ]
    }
   ],
   "source": [
    "## basic logistic reg\n",
    "xg_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:logistic\",\n",
    "    colsample_bytree=0.3,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    alpha=10,\n",
    "    n_estimators=10,\n",
    ")\n",
    "xg_reg.fit(X_train, y_train)\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0          0.464961        0.000004        0.464989       0.000018\n",
      "1          0.434851        0.000135        0.434908       0.000167\n",
      "2          0.408743        0.000077        0.408827       0.000064\n",
      "3          0.386510        0.000091        0.386617       0.000067\n",
      "4          0.367159        0.000308        0.367300       0.000203\n",
      "5          0.350742        0.000486        0.350905       0.000341\n",
      "6          0.336828        0.000541        0.337004       0.000384\n",
      "7          0.324850        0.000453        0.325060       0.000286\n",
      "8          0.314785        0.000628        0.315026       0.000451\n",
      "9          0.306553        0.000666        0.306818       0.000478\n",
      "10         0.299667        0.000626        0.299954       0.000422\n",
      "11         0.293873        0.000637        0.294169       0.000425\n",
      "12         0.288953        0.000684        0.289277       0.000451\n",
      "13         0.284756        0.000602        0.285101       0.000358\n",
      "14         0.281273        0.000570        0.281651       0.000316\n",
      "15         0.278419        0.000424        0.278809       0.000183\n",
      "16         0.275935        0.000229        0.276362       0.000049\n",
      "17         0.273928        0.000363        0.274387       0.000290\n",
      "18         0.272227        0.000377        0.272710       0.000415\n",
      "19         0.270885        0.000262        0.271391       0.000408\n",
      "20         0.269802        0.000384        0.270321       0.000496\n",
      "21         0.268750        0.000227        0.269285       0.000372\n",
      "22         0.267894        0.000300        0.268454       0.000347\n",
      "23         0.267144        0.000405        0.267743       0.000375\n",
      "24         0.266444        0.000366        0.267059       0.000359\n",
      "25         0.266030        0.000336        0.266658       0.000348\n",
      "26         0.265557        0.000304        0.266211       0.000402\n",
      "27         0.265049        0.000295        0.265738       0.000494\n",
      "28         0.264685        0.000316        0.265398       0.000546\n",
      "29         0.264376        0.000317        0.265116       0.000518\n",
      "30         0.264095        0.000396        0.264863       0.000605\n",
      "31         0.263945        0.000404        0.264730       0.000624\n",
      "32         0.263716        0.000413        0.264534       0.000667\n",
      "33         0.263454        0.000497        0.264302       0.000740\n",
      "34         0.263147        0.000497        0.264026       0.000742\n",
      "35         0.262986        0.000480        0.263876       0.000716\n",
      "36         0.262730        0.000352        0.263650       0.000573\n",
      "37         0.262636        0.000325        0.263582       0.000539\n",
      "38         0.262499        0.000349        0.263473       0.000545\n",
      "39         0.262352        0.000380        0.263351       0.000573\n",
      "40         0.262209        0.000357        0.263224       0.000561\n",
      "41         0.262100        0.000351        0.263137       0.000571\n",
      "42         0.261976        0.000371        0.263046       0.000611\n",
      "43         0.261857        0.000407        0.262964       0.000644\n",
      "44         0.261679        0.000289        0.262812       0.000530\n",
      "45         0.261521        0.000202        0.262686       0.000439\n",
      "46         0.261427        0.000174        0.262616       0.000423\n",
      "47         0.261317        0.000125        0.262525       0.000368\n",
      "48         0.261240        0.000148        0.262477       0.000377\n",
      "49         0.261108        0.000114        0.262371       0.000332\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"reg:logistic\",\n",
    "    \"colsample_bytree\": 0.3,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 5,\n",
    "    \"alpha\": 10,\n",
    "}\n",
    "cv_results = xgb.cv(\n",
    "    dtrain=data_matrix,\n",
    "    params=params,\n",
    "    nfold=3,\n",
    "    num_boost_round=50,\n",
    "    early_stopping_rounds=10,\n",
    "    metrics=\"rmse\",\n",
    "    as_pandas=True,\n",
    "    seed=69,\n",
    ")\n",
    "\n",
    "# print((cv_results[\"test-rmse-mean\"]).tail(1))\n",
    "print(cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | colsam... | colsam... |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "def xgb_evaluation(\n",
    "    max_depth,\n",
    "    min_child_weight,\n",
    "    gamma,\n",
    "    subsample,\n",
    "    colsample_bytree,\n",
    "    colsample_bylevel,\n",
    "    colsample_bynode,\n",
    "    reg_alpha,\n",
    "    reg_lambda,\n",
    "):\n",
    "\n",
    "    params = {\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"max_depth\": int(round(max_depth)),\n",
    "        \"min_child_weight\": int(round(min_child_weight)),\n",
    "        \"subsample\": subsample,\n",
    "        \"gamma\": gamma,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"colsample_bylevel\": colsample_bylevel,\n",
    "        \"colsample_bynode\": colsample_bynode,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"random_state\": 51412,\n",
    "    }\n",
    "\n",
    "    # defining the Cross-Validation Strategry\n",
    "    stratified_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=33)\n",
    "    cv_preds = np.zeros(reduced_train_df.shape[0])\n",
    "\n",
    "    # iterating over each fold, training the model, and making Out of Fold Predictions\n",
    "    for train_indices, cv_indices in stratified_cv.split(reduced_train_df, target_df):\n",
    "\n",
    "        x_tr = reduced_train_df.iloc[train_indices]\n",
    "        y_tr = target_df.iloc[train_indices]\n",
    "        x_cv = reduced_train_df.iloc[cv_indices]\n",
    "        y_cv = target_df.iloc[cv_indices]\n",
    "\n",
    "        xgbc = XGBClassifier(**params)\n",
    "        xgbc.fit(\n",
    "            x_tr,\n",
    "            y_tr,\n",
    "            eval_set=[(x_cv, y_cv)],\n",
    "            eval_metric=\"auc\",\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=200,\n",
    "        )\n",
    "\n",
    "        cv_preds[cv_indices] = xgbc.predict_proba(\n",
    "            x_cv, ntree_limit=xgbc.get_booster().best_ntree_limit\n",
    "        )[:, 1]\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(target_df, cv_preds)\n",
    "\n",
    "\n",
    "bopt_xgb = BayesianOptimization(\n",
    "    xgb_evaluation,\n",
    "    {\n",
    "        \"max_depth\": (5, 15),\n",
    "        \"min_child_weight\": (5, 80),\n",
    "        \"gamma\": (0.2, 1),\n",
    "        \"subsample\": (0.5, 1),\n",
    "        \"colsample_bytree\": (0.5, 1),\n",
    "        \"colsample_bylevel\": (0.3, 1),\n",
    "        \"colsample_bynode\": (0.3, 1),\n",
    "        \"reg_alpha\": (0.001, 0.3),\n",
    "        \"reg_lambda\": (0.001, 0.3),\n",
    "    },\n",
    "    random_state=55,\n",
    ")\n",
    "bopt_xgb.maximize(n_iter=6, init_points=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
