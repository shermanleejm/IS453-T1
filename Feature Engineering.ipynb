{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "## on mac run `brew install lightgbm` \n",
    "## anythign else refer here https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html#apple-clang\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./data/application_train.csv\")\n",
    "test_df = pd.read_csv(\"./data/application_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-286c6ff8d1a2>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['DAYS_EMPLOYED'][train_df['DAYS_EMPLOYED'] == 365243] = np.nan\n",
      "<ipython-input-60-286c6ff8d1a2>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['DAYS_EMPLOYED'][test_df['DAYS_EMPLOYED'] == 365243] = np.nan\n"
     ]
    }
   ],
   "source": [
    "## clean up some rows\n",
    "train_df['DAYS_EMPLOYED'][train_df['DAYS_EMPLOYED'] == 365243] = np.nan\n",
    "test_df['DAYS_EMPLOYED'][test_df['DAYS_EMPLOYED'] == 365243] = np.nan\n",
    "train_df['OBS_30_CNT_SOCIAL_CIRCLE'][train_df['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "train_df['OBS_60_CNT_SOCIAL_CIRCLE'][train_df['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "test_df['OBS_30_CNT_SOCIAL_CIRCLE'][test_df['OBS_30_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "test_df['OBS_60_CNT_SOCIAL_CIRCLE'][test_df['OBS_60_CNT_SOCIAL_CIRCLE'] > 30] == np.nan\n",
    "train_df = train_df[train_df['CODE_GENDER'] != 'XNA']\n",
    "\n",
    "## remove rows with only 1 distinct value\n",
    "empty_columns = []\n",
    "for col in train_df.columns:\n",
    "    if len(train_df[col].unique()) <=1:\n",
    "        empty_columns.append(col)\n",
    "train_df = train_df.drop(empty_columns, axis = 1)\n",
    "test_df = test_df.drop(empty_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## other pre-processing\n",
    "train_df[\"DAYS_BIRTH\"] = train_df[\"DAYS_BIRTH\"] * -1 / 365\n",
    "test_df[\"DAYS_BIRTH\"] = test_df[\"DAYS_BIRTH\"] * -1 / 365\n",
    "categorical_columns = train_df.dtypes[train_df.dtypes == \"object\"].index.tolist()\n",
    "train_df[categorical_columns] = train_df[categorical_columns].fillna(\"XNA\")\n",
    "test_df[categorical_columns] = test_df[categorical_columns].fillna(\"XNA\")\n",
    "train_df[\"REGION_RATING_CLIENT\"] = train_df[\"REGION_RATING_CLIENT\"].astype(\"object\")\n",
    "train_df[\"REGION_RATING_CLIENT_W_CITY\"] = train_df[\n",
    "    \"REGION_RATING_CLIENT_W_CITY\"\n",
    "].astype(\"object\")\n",
    "test_df[\"REGION_RATING_CLIENT\"] = test_df[\"REGION_RATING_CLIENT\"].astype(\"object\")\n",
    "test_df[\"REGION_RATING_CLIENT_W_CITY\"] = test_df[\"REGION_RATING_CLIENT_W_CITY\"].astype(\n",
    "    \"object\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shitty self made encoder\n",
    "encoder_dict = {}\n",
    "for col in list(train_df.columns):\n",
    "    if str(train_df[col].dtypes) == \"object\":\n",
    "        tmp = list(train_df[col].unique())\n",
    "        encoder = {name: i for i, name in enumerate(tmp)}\n",
    "        encoder_dict[col] = encoder\n",
    "        train_df[col] = train_df[col].apply(lambda x: encoder[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307507, 122)\n",
      "(48744, 121)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "target_df = train_df.pop(\"TARGET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## using LightGBM for recursive feature selection\n",
    "def cut_down_features(train_df, test_df):\n",
    "    num_folds = 3\n",
    "    impt_cols = set()\n",
    "    score = 1\n",
    "    i = 1\n",
    "    while score > 0.72:\n",
    "        selection_data = train_df.drop(list(impt_cols), axis=1)\n",
    "        fold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=33)\n",
    "        score = 0\n",
    "        model_feature_importance = np.zeros_like(selection_data.columns)\n",
    "        for fold_num, (train_indices, val_indices) in enumerate(\n",
    "            fold.split(selection_data, target_df), 1\n",
    "        ):\n",
    "            x_train = selection_data.iloc[train_indices]\n",
    "            x_val = selection_data.iloc[val_indices]\n",
    "            y_train = target_df.iloc[train_indices]\n",
    "            y_val = target_df.iloc[val_indices]\n",
    "            lg = LGBMClassifier(n_jobs=-1, random_state=69)\n",
    "            lg.fit(x_train, y_train)\n",
    "            model_feature_importance += lg.feature_importances_ / num_folds\n",
    "            score += roc_auc_score(y_val, lg.predict_proba(x_val)[:, 1]) / num_folds\n",
    "        imp_cols_indices = np.where(np.abs(model_feature_importance) > 0)\n",
    "        cols_imp = train_df.columns[imp_cols_indices]\n",
    "        if score > 0.7:\n",
    "            impt_cols.update(cols_imp)\n",
    "        i += 1\n",
    "    impt_cols = list(impt_cols)\n",
    "    train_df = train_df[impt_cols]\n",
    "    test_df = test_df[impt_cols]\n",
    "    with open(\"final_cols.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_df.columns.tolist(), f)\n",
    "    gc.collect()\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "reduced_train_df, reduced_test_df = cut_down_features(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n",
      "109\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.columns))\n",
    "print(len(reduced_train_df.columns))\n",
    "# lmao\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    reduced_train_df, target_df, test_size=0.2\n",
    ")\n",
    "\n",
    "data_matrix = xgb.DMatrix(data=reduced_train_df, label=target_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0          0.464961        0.000004        0.464989       0.000018\n",
      "1          0.434851        0.000135        0.434908       0.000167\n",
      "2          0.408743        0.000077        0.408827       0.000064\n",
      "3          0.386510        0.000091        0.386617       0.000067\n",
      "4          0.367159        0.000308        0.367300       0.000203\n",
      "5          0.350742        0.000486        0.350905       0.000341\n",
      "6          0.336828        0.000541        0.337004       0.000384\n",
      "7          0.324850        0.000453        0.325060       0.000286\n",
      "8          0.314785        0.000628        0.315026       0.000451\n",
      "9          0.306553        0.000666        0.306818       0.000478\n",
      "10         0.299667        0.000626        0.299954       0.000422\n",
      "11         0.293873        0.000637        0.294169       0.000425\n",
      "12         0.288953        0.000684        0.289277       0.000451\n",
      "13         0.284756        0.000602        0.285101       0.000358\n",
      "14         0.281273        0.000570        0.281651       0.000316\n",
      "15         0.278419        0.000424        0.278809       0.000183\n",
      "16         0.275935        0.000229        0.276362       0.000049\n",
      "17         0.273928        0.000363        0.274387       0.000290\n",
      "18         0.272227        0.000377        0.272710       0.000415\n",
      "19         0.270885        0.000262        0.271391       0.000408\n",
      "20         0.269802        0.000384        0.270321       0.000496\n",
      "21         0.268750        0.000227        0.269285       0.000372\n",
      "22         0.267894        0.000300        0.268454       0.000347\n",
      "23         0.267144        0.000405        0.267743       0.000375\n",
      "24         0.266444        0.000366        0.267059       0.000359\n",
      "25         0.266030        0.000336        0.266658       0.000348\n",
      "26         0.265557        0.000304        0.266211       0.000402\n",
      "27         0.265049        0.000295        0.265738       0.000494\n",
      "28         0.264685        0.000316        0.265398       0.000546\n",
      "29         0.264376        0.000317        0.265116       0.000518\n",
      "30         0.264095        0.000396        0.264863       0.000605\n",
      "31         0.263945        0.000404        0.264730       0.000624\n",
      "32         0.263716        0.000413        0.264534       0.000667\n",
      "33         0.263454        0.000497        0.264302       0.000740\n",
      "34         0.263147        0.000497        0.264026       0.000742\n",
      "35         0.262986        0.000480        0.263876       0.000716\n",
      "36         0.262730        0.000352        0.263650       0.000573\n",
      "37         0.262636        0.000325        0.263582       0.000539\n",
      "38         0.262499        0.000349        0.263473       0.000545\n",
      "39         0.262352        0.000380        0.263351       0.000573\n",
      "40         0.262209        0.000357        0.263224       0.000561\n",
      "41         0.262100        0.000351        0.263137       0.000571\n",
      "42         0.261976        0.000371        0.263046       0.000611\n",
      "43         0.261857        0.000407        0.262964       0.000644\n",
      "44         0.261679        0.000289        0.262812       0.000530\n",
      "45         0.261521        0.000202        0.262686       0.000439\n",
      "46         0.261427        0.000174        0.262616       0.000423\n",
      "47         0.261317        0.000125        0.262525       0.000368\n",
      "48         0.261240        0.000148        0.262477       0.000377\n",
      "49         0.261108        0.000114        0.262371       0.000332\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"objective\": \"reg:logistic\",\n",
    "    \"colsample_bytree\": 0.3,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 5,\n",
    "    \"alpha\": 10,\n",
    "}\n",
    "cv_results = xgb.cv(\n",
    "    dtrain=data_matrix,\n",
    "    params=params,\n",
    "    nfold=3,\n",
    "    num_boost_round=50,\n",
    "    early_stopping_rounds=10,\n",
    "    metrics=\"rmse\",\n",
    "    as_pandas=True,\n",
    "    seed=69,\n",
    ")\n",
    "\n",
    "# print((cv_results[\"test-rmse-mean\"]).tail(1))\n",
    "print(cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | colsam... | colsam... |   gamma   | max_depth | min_ch... | reg_alpha | reg_la... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7594  \u001b[0m | \u001b[0m 0.3652  \u001b[0m | \u001b[0m 0.9802  \u001b[0m | \u001b[0m 0.7419  \u001b[0m | \u001b[0m 0.394   \u001b[0m | \u001b[0m 10.31   \u001b[0m | \u001b[0m 26.42   \u001b[0m | \u001b[0m 0.2589  \u001b[0m | \u001b[0m 0.01329 \u001b[0m | \u001b[0m 0.5542  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.7603  \u001b[0m | \u001b[95m 0.837   \u001b[0m | \u001b[95m 0.336   \u001b[0m | \u001b[95m 0.8879  \u001b[0m | \u001b[95m 0.2073  \u001b[0m | \u001b[95m 11.18   \u001b[0m | \u001b[95m 66.4    \u001b[0m | \u001b[95m 0.2697  \u001b[0m | \u001b[95m 0.2957  \u001b[0m | \u001b[95m 0.7484  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.7608  \u001b[0m | \u001b[95m 0.5466  \u001b[0m | \u001b[95m 0.907   \u001b[0m | \u001b[95m 0.6984  \u001b[0m | \u001b[95m 0.7149  \u001b[0m | \u001b[95m 5.205   \u001b[0m | \u001b[95m 65.61   \u001b[0m | \u001b[95m 0.1295  \u001b[0m | \u001b[95m 0.168   \u001b[0m | \u001b[95m 0.8896  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7593  \u001b[0m | \u001b[0m 0.5818  \u001b[0m | \u001b[0m 0.9356  \u001b[0m | \u001b[0m 0.825   \u001b[0m | \u001b[0m 0.7818  \u001b[0m | \u001b[0m 12.8    \u001b[0m | \u001b[0m 31.3    \u001b[0m | \u001b[0m 0.006878\u001b[0m | \u001b[0m 0.1082  \u001b[0m | \u001b[0m 0.6516  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7605  \u001b[0m | \u001b[0m 0.3558  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 55.96   \u001b[0m | \u001b[0m 0.1372  \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7603  \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 80.0    \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7601  \u001b[0m | \u001b[0m 0.4323  \u001b[0m | \u001b[0m 0.8145  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.2     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7605  \u001b[0m | \u001b[0m 0.3     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.5     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 71.05   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 0.001   \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7602  \u001b[0m | \u001b[0m 0.5097  \u001b[0m | \u001b[0m 0.806   \u001b[0m | \u001b[0m 0.9259  \u001b[0m | \u001b[0m 0.3875  \u001b[0m | \u001b[0m 14.92   \u001b[0m | \u001b[0m 79.76   \u001b[0m | \u001b[0m 0.2212  \u001b[0m | \u001b[0m 0.2004  \u001b[0m | \u001b[0m 0.7468  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "/usr/local/lib/python3.9/site-packages/xgboost/core.py:101: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.7605  \u001b[0m | \u001b[0m 0.7834  \u001b[0m | \u001b[0m 0.4313  \u001b[0m | \u001b[0m 0.8429  \u001b[0m | \u001b[0m 0.9665  \u001b[0m | \u001b[0m 5.07    \u001b[0m | \u001b[0m 61.72   \u001b[0m | \u001b[0m 0.127   \u001b[0m | \u001b[0m 0.1321  \u001b[0m | \u001b[0m 0.7314  \u001b[0m |\n",
      "=====================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "def xgb_evaluation(\n",
    "    max_depth,\n",
    "    min_child_weight,\n",
    "    gamma,\n",
    "    subsample,\n",
    "    colsample_bytree,\n",
    "    colsample_bylevel,\n",
    "    colsample_bynode,\n",
    "    reg_alpha,\n",
    "    reg_lambda,\n",
    "):\n",
    "\n",
    "    params = {\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"n_estimators\": 10000,\n",
    "        \"max_depth\": int(round(max_depth)),\n",
    "        \"min_child_weight\": int(round(min_child_weight)),\n",
    "        \"subsample\": subsample,\n",
    "        \"gamma\": gamma,\n",
    "        \"colsample_bytree\": colsample_bytree,\n",
    "        \"colsample_bylevel\": colsample_bylevel,\n",
    "        \"colsample_bynode\": colsample_bynode,\n",
    "        \"reg_alpha\": reg_alpha,\n",
    "        \"reg_lambda\": reg_lambda,\n",
    "        \"random_state\": 51412,\n",
    "    }\n",
    "\n",
    "    # defining the Cross-Validation Strategry\n",
    "    stratified_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=33)\n",
    "    cv_preds = np.zeros(reduced_train_df.shape[0])\n",
    "\n",
    "    # iterating over each fold, training the model, and making Out of Fold Predictions\n",
    "    for train_indices, cv_indices in stratified_cv.split(reduced_train_df, target_df):\n",
    "\n",
    "        x_tr = reduced_train_df.iloc[train_indices]\n",
    "        y_tr = target_df.iloc[train_indices]\n",
    "        x_cv = reduced_train_df.iloc[cv_indices]\n",
    "        y_cv = target_df.iloc[cv_indices]\n",
    "\n",
    "        xgbc = XGBClassifier(**params)\n",
    "        xgbc.fit(\n",
    "            x_tr,\n",
    "            y_tr,\n",
    "            eval_set=[(x_cv, y_cv)],\n",
    "            eval_metric=\"auc\",\n",
    "            verbose=False,\n",
    "            early_stopping_rounds=200,\n",
    "        )\n",
    "\n",
    "        cv_preds[cv_indices] = xgbc.predict_proba(\n",
    "            x_cv, ntree_limit=xgbc.get_booster().best_ntree_limit\n",
    "        )[:, 1]\n",
    "        gc.collect()\n",
    "\n",
    "    return roc_auc_score(target_df, cv_preds)\n",
    "\n",
    "\n",
    "bopt_xgb = BayesianOptimization(\n",
    "    xgb_evaluation,\n",
    "    {\n",
    "        \"max_depth\": (5, 15),\n",
    "        \"min_child_weight\": (5, 80),\n",
    "        \"gamma\": (0.2, 1),\n",
    "        \"subsample\": (0.5, 1),\n",
    "        \"colsample_bytree\": (0.5, 1),\n",
    "        \"colsample_bylevel\": (0.3, 1),\n",
    "        \"colsample_bynode\": (0.3, 1),\n",
    "        \"reg_alpha\": (0.001, 0.3),\n",
    "        \"reg_lambda\": (0.001, 0.3),\n",
    "    },\n",
    "    random_state=55,\n",
    ")\n",
    "bopt_xgb.maximize(n_iter=6, init_points=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best params\n",
    "params = bopt_xgb.max['params']\n",
    "## convert params to int\n",
    "for k in [\"max_depth\", 'min_child_weight']:\n",
    "    params[k] = int(params[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bylevel': 0.5466232480392754,\n",
       " 'colsample_bynode': 0.9069594961304306,\n",
       " 'colsample_bytree': 0.698440872861886,\n",
       " 'gamma': 0.7149257753413591,\n",
       " 'max_depth': 5,\n",
       " 'min_child_weight': 65,\n",
       " 'reg_alpha': 0.1294543773037284,\n",
       " 'reg_lambda': 0.16800541022842516,\n",
       " 'subsample': 0.8895556450751974}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.2598874274677816\n",
      "Accuracy: 0.9200351208090793\n"
     ]
    }
   ],
   "source": [
    "## basic logistic reg\n",
    "xg_reg = xgb.XGBRegressor(objective=\"reg:logistic\", **params)\n",
    "xg_reg.fit(X_train, y_train)\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"RMSE: {rmse}\")\n",
    "accuracy = accuracy_score(y_test, [round(i) for i in y_pred])\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
